In this step, we will check the state Kafka broker once it has moved to a new node. Note that when it starts it will have all messages already on disk because we have been using Portworx replication to synchronously replicate data to 3 nodes. In large scale scenarios where you have multiple brokers and are using Kafka replication for high availability the broker would synch only those messages that it missed while it was down from the other brokers in the cluster. Without Portworx in that scenario the synching process would be much longer and would cause a lot of network traffic. Read more about this topic in this blog post [TODO:link to Kafka blog post](http://google.com)

### Step: Verify data is still available

Run the client and in a shell.
```
 POD=`kubectl get pods | grep 'px-kafka-' | awk '{print $1}'`
 kubectl exec -it $POD -- kafka --host px-kafka-kafkadb
```{{execute T1}}

Now use the kafka shell to make sure our data is still there.
Find one arbitrary document:
```
db.ships.findOne()
```{{execute T1}}
Find all documents and using nice formatting:
```
db.ships.find().pretty()
```{{execute T1}}
Shows only the names of the ships:
```
db.ships.find({}, {name:true, _id:false})
```{{execute T1}}
Finds one document by attribute:
```
db.ships.findOne({'name':'USS Defiant'})
```{{execute T1}}

Our data is still there! Portworx replication makes handling kafkaDB a HA easy. Exit from the client shell before continuing: ```exit```{{execute T1}}
