In this step, we will check the state of our sample Cassandra database.


### Step: Restore the snapshot and see your data is still there

First we will delete the stateful set and the PVCs so that we can start a new cluster in it's place. Note that you could just create a new cluster with a different name in parallel but that our Katacoda cluster doesn't have enough resources.
```
kubectl delete -f cassandra.yaml
```{{execute T1}}

Now delete the PVCs:
```
kubectl delete pvc cassandra-data-cassandra-0
kubectl delete pvc cassandra-data-cassandra-1
kubectl delete pvc cassandra-data-cassandra-2
```{{execute T1}}

Now let's create three new PVCs from our snapshots:
```
cat i=0
for snap in $(kubectl get volumesnapshotdatas | grep px-cassandra | awk '{print $1}'); do cat px-snap-pvc.yaml | seds/px-cassandra-snapshot/$snap/g | sed s/px-cassandra-snap-clone/cassandra-data-cassandra-$i/g > cassandra-pvc-$i.yaml; kubectl create -f cassandra-pvc-$i.yaml; i=$(($i+1)); done;
```{{execute T1}}

Now we can recreate our Cassandra cluster and it will use the PVCs we created:
```
kubectl create -f cassandra.yaml
kubectl scale --replicas=3 sts cassandra
```{{execute T1}}

Watch to see the cassandra pods come back with the restored volumes.
```
watch kubectl get pods
```{{execute T1}}

When the pod is in Running state then then hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

### Step: Verify data is still available

Start a CQL Shell session:
```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
```{{execute T1}}

Select rows from the keyspace we previously created:
```
SELECT id, name, value FROM portworx.features;
```{{execute T1}}

Now that we have verify our data survived the node failure let's ```quit```{{execute T1}} the cqlsh session.
