In this step, we will take a snapshot of our Kafka volumes and show how it can be recovered from the snapshot.

### Step: Take snapshot using kubectl

First, take a look at px-snap.yaml ```cat px-snap.yaml```{{execute T1}}. Notice that when you define the volume group name Portworx will quiesce I/O on all volumes before triggering their snapshots.

```
kubectl create -f px-snap.yaml
```{{execute T1}}

You can see the snapshots using the following command:
```
kubectl get volumesnapshot,volumesnapshotdatas
```{{execute T1}}

You can also look at the Portworx command line view of all 6 initial volumes and the 3 snapshots:
```
PX_POD=$(kubectl get pods -l name=portworx -n kube-system -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $PX_POD -n kube-system -- /opt/pwx/bin/pxctl volume list --snapshot
```{{execute T1}}

Now we're going to go ahead and do something stupid because it's Katacoda and we're here to learn.

```
kubectl exec -it kafka-cli bash
./bin/kafka-topics.sh --zookeeper zk-headless:2181 --delete --topic test
./bin/kafka-topics.sh --zookeeper zk-headless:2181 --list
```{{execute T1}}

Ok, so we deleted our topic, what now? Restore your snapshot and cary on.

### Step: Restore the snapshot and see your data is still there

Snapshots are just like volumes so we can go ahead and use it to start a new instance of Kafka.

First, lets scale down our Kafka cluster:
```
kubectl scale sts kafka --replicas=0
```{{execute T1}}

Make sure you wait for all three kafka pods to be deleted:
```
watch kubectl get pods
```{{execute T1}}
Now let's restore the snapshot group volumes:
```
PVC_LIST=`kubectl get pvc | grep kafka | awk '{print $3}'`
kubectl exec -it $PX_POD -n kube-system -- /opt/pwx/bin/pxctl v l -s > out.txt
SNAP_PREFIX=`cat out.txt | grep group | sed '1q;d' | awk '{print $2}' | awk '{print substr($1,0,index($1,"_pvc"))}'`
for pvc in $PVC_LIST; do kubectl exec -it $PX_POD -n kube-system -- /opt/pwx/bin/pxctl v restore -s ${SNAP_PREFIX}_$pvc $pvc; done
```{{execute T1}}

### Step: Scale your StatefulSet back up to 3 and verify your test topic is back
```
kubectl scale sts kafka --replicas=3
```{{execute T1}}

Make sure you wait for all three kafka pods to be started and ready:
```
watch kubectl get pods
```{{execute T1}}

When all three kafka-* pods are ready hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen and then verify that your test topic is back and has the full data history:
```
./bin/kafka-console-consumer.sh --bootstrap-server kafka-broker:9092 --topic test --partition 0 --from-beginning
```{{execute T1}}
